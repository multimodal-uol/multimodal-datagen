{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import build\n",
    "from newspaper import Article\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import traceback\n",
    "import requests\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#outputdir = '/opt/newspaperdata/news/'\n",
    "outputdir = '/tmp/'\n",
    "\n",
    "inputfile= 'list-websites.txt'\n",
    "listArticles = []\n",
    "publisher=''\n",
    "\n",
    "\n",
    "\n",
    "class articleObj:\n",
    "    def __init__(self, title, date_published, news_outlet,\n",
    "                 feature_img, article_link, keywords,movies, Summary, text):\n",
    "        self.title = title\n",
    "        self.date_published = date_published\n",
    "        self.news_outlet = news_outlet\n",
    "        self.feature_img = feature_img\n",
    "        self.article_link = article_link\n",
    "        self.keywords = keywords\n",
    "        self.movies = movies\n",
    "        self.Summary = Summary\n",
    "        self.text = text\n",
    "     \n",
    "\n",
    "def website_parser (article, publisher, dateToday):\n",
    "    movies =[]\n",
    "    date_published = dateToday\n",
    "    title = article.title,\n",
    "    website = publisher\n",
    "    feature_img =  article.top_image\n",
    "    article_link = article.url\n",
    "    keywords = article.keywords\n",
    "    \n",
    "    #extract videos\n",
    "    r = requests.get(article.url)\n",
    "    soup = BeautifulSoup(r.content,'html.parser')\n",
    "    links = soup.find_all('iframe')\n",
    "    for link in links:\n",
    "        try:\n",
    "            video_link = link['src']\n",
    "            #video_link = link['video']\n",
    "        except Exception:\n",
    "            continue\n",
    "        if video_link:\n",
    "            if  not(('googletagmanager' in video_link) or ('ServiceLogin' in video_link)) :\n",
    "                print('video_link '+video_link)\n",
    "                movies.append(video_link)\n",
    "    summary = article.summary\n",
    "    text = article.text\n",
    "    anArticle= articleObj (title, date_published, website, \n",
    "                                    feature_img, article_link, keywords,\n",
    "                                    movies, summary, text)\n",
    "    listArticles.append(anArticle)\n",
    "\n",
    "def write_to_json (file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        if listArticles:\n",
    "            json.dump([ob.__dict__ for ob in listArticles], f)\n",
    "\n",
    "def scrape_url ():\n",
    "    with open(inputfile) as f:\n",
    "         lines = [line.rstrip() for line in f]\n",
    "    for line in lines:\n",
    "        publisher=''\n",
    "        global listArticles\n",
    "        listArticles = []\n",
    "        i = 0\n",
    "        append_url = ''\n",
    "        base_url = ''\n",
    "        \n",
    "        articles = []\n",
    "        page = requests.get(line)    \n",
    "        data = page.text\n",
    "        soup = BeautifulSoup(data)\n",
    "        exclude = ['/a-z', '/LoginPage', '/privacy', '/support', \n",
    "                   '/contact-us','/alexa','/referrals','/browse/','/help',\n",
    "                   '/sitemap','/cookies','/language-policy','/personal-data-protection',\n",
    "                   '/copyright','/our-policies','/all-topics','/news-events','/publications-data',\n",
    "                   '/data-tools','about-us','mailto','/terms-conditions','/privacy-notice',\n",
    "                   '/working','/visas-immigration','/abroad','/tax','/housing-local-services',\n",
    "                   '/environment-countryside','/employing-people','/education','/driving','/disabilities',\n",
    "                   '/justice','/citizenship','/childcare-parenting','/business','/births-deaths-marriages',\n",
    "                   '/benefits','/contact','/terms-conditions','/who-brochure','/footer','/nhs-sites/','/login','sitemap/',\n",
    "                   '/accessibility-statement/','/our-policies/','/jobs','/libraries','/redirect-pages']\n",
    "\n",
    "        for link in soup.find_all('a'):\n",
    "            alink = link.get('href')\n",
    "            if not any(ex in alink for ex in exclude):\n",
    "                articles.append(alink)\n",
    "        articles.append(line)\n",
    "                \n",
    "        if line == 'https://www.gov.uk/coronavirus' :\n",
    "            base_url = 'https://www.gov.uk'\n",
    "        elif line == 'https://www.nhs.uk/conditions/coronavirus-covid-19' :\n",
    "            base_url = 'https://www.nhs.uk'\n",
    "        elif line == 'https://www.gov.uk/government/organisations/scientific-advisory-group-for-emergencies' :\n",
    "            base_url = 'https://www.gov.uk'\n",
    "        elif line == 'https://www.cdc.gov/coronavirus/2019-ncov/index.html' :\n",
    "            base_url = 'https://www.cdc.gov'\n",
    "        elif line == 'https://www.who.int/emergencies/diseases/novel-coronavirus-2019':\n",
    "            base_url = 'https://www.who.int'\n",
    "        elif line == 'https://www.nhs.uk/conditions/coronavirus-covid-19' :\n",
    "            base_url = 'https://www.nhs.uk'\n",
    "        else :\n",
    "            base_url = line\n",
    "            \n",
    "        for article in articles:\n",
    "            if article : \n",
    "                if  not(('http://'  in article) or ('https://' in article)) :\n",
    "                    article = base_url + article \n",
    "            print('URL '+article)\n",
    "            article = Article(article)\n",
    "            try:\n",
    "                article.build()\n",
    "            except Exception:\n",
    "                print(traceback.format_exc())\n",
    "                #time.sleep(60)\n",
    "                continue\n",
    "            data = website_parser(article, line, dateToday)\n",
    "        write_to_json('/tmp/'+dateNow+'/'+line.replace('https://www','').replace('.','').replace('/','')\n",
    "                      .replace(':','').replace('https','')\n",
    "                      +datetime.today().strftime('%Y-%m-%d').replace(' ','T')+'.json')\n",
    "        \n",
    "        \n",
    "dateNow = datetime.now().strftime(\"%H%M%S%f\")\n",
    "dateToday = datetime.today().strftime('%Y-%m-%d')\n",
    "if  not os.path.exists('/tmp/'+dateNow):\n",
    "    os.mkdir('/tmp/'+dateNow)\n",
    "scrape_url()\n",
    "glob_data = []\n",
    "for file in glob.glob('/tmp/'+dateNow+'/'+'*.json'):\n",
    "    if os.path.getsize(file) > 0 :\n",
    "        with open(file) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            i = 0\n",
    "            while i < len(data):\n",
    "                glob_data.append(data[i])\n",
    "                i += 1              \n",
    "with open(outputdir+dateToday+'.json', 'w') as f:\n",
    "    json.dump(glob_data, f, indent=4)\n",
    "    \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
